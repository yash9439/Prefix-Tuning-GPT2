{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "PROMPT_TOKEN = \"[TRANSLATE]\"\n",
    "MAX_LEN = 500\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"[TRANSLATE]\"]  # Define your custom vocabulary here\n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "\n",
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def load_data_from_files(english_file, german_file):\n",
    "    with open(english_file, \"r\", encoding=\"utf-8\") as eng_file:\n",
    "        english_list = eng_file.readlines()\n",
    "    \n",
    "    with open(german_file, \"r\", encoding=\"utf-8\") as ger_file:\n",
    "        german_list = ger_file.readlines()\n",
    "\n",
    "    return english_list[:600], german_list[:600]  \n",
    "\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(english_file, german_file, num_prompts):\n",
    "    english_list, german_list = load_data_from_files(english_file, german_file)\n",
    "\n",
    "    # Perform preprocessing on the data\n",
    "    tokenized_english = []\n",
    "    tokenized_german = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for english_sentence, german_sentence in zip(english_list, german_list):\n",
    "        english_tokens = tokenizer.encode(english_sentence, truncation=True, max_length=MAX_LEN)\n",
    "        german_tokens = tokenizer.encode(german_sentence, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "        # Pad the sequences to MAX_LEN\n",
    "        padded_english = english_tokens + [tokenizer.eos_token_id] * (MAX_LEN-1 - len(english_tokens))\n",
    "        padded_german = german_tokens + [tokenizer.eos_token_id] * (MAX_LEN - len(german_tokens))\n",
    "\n",
    "        tokenized_english.append(padded_english)\n",
    "        tokenized_german.append(padded_german)\n",
    "\n",
    "    return tokenized_english, tokenized_german\n",
    "\n",
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenized_articles_total , tokenized_summaries_total = load_and_preprocess_data(\"europarl-v7.de-en.en\", \"europarl-v7.de-en.de\",num_prompts)\n",
    "total_samples = len(tokenized_articles_total)\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "# Calculate the sizes of the three sets\n",
    "train_size = int(total_samples * split_ratio[0])\n",
    "val_size = int(total_samples * split_ratio[1])\n",
    "test_size = int(total_samples * split_ratio[2])\n",
    "\n",
    "# Split the data\n",
    "tokenized_articles_train = tokenized_articles_total[:train_size]\n",
    "tokenized_summaries_train = tokenized_summaries_total[:train_size]\n",
    "\n",
    "tokenized_articles_validation = tokenized_articles_total[train_size:train_size + val_size]\n",
    "tokenized_summaries_validation = tokenized_summaries_total[train_size:train_size + val_size]\n",
    "\n",
    "tokenized_articles_test = tokenized_articles_total[train_size + val_size:]\n",
    "tokenized_summaries_test = tokenized_summaries_total[train_size + val_size:]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# # Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "prompt_id = prompt_id.to(device)\n",
    "# Import cross_entropy_loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def fine_tune_on_summarization(model, train_articles, train_summaries, val_articles, val_summaries, test_articles, test_summaries):\n",
    "    optimizer = torch.optim.Adam(model.soft_prompt.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        # Gradient accumulation initialization\n",
    "        optimizer.zero_grad()\n",
    "        accumulated_loss = 0\n",
    "        loss = 0\n",
    "        # Use tqdm for progress bar\n",
    "        with tqdm(enumerate(zip(train_articles, train_summaries)), total=len(train_articles), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n",
    "            train_percentage_matched = 0\n",
    "            train_percentage_matched_ct = 0\n",
    "            train_pred_sentences = []\n",
    "            train_true_sentences = []\n",
    "            for idx, (article, summary) in progress:\n",
    "                input_ids = torch.tensor(article).to(device)\n",
    "                labels = torch.tensor(summary).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                # Bleu Score\n",
    "                pred_logits = outputs.logits\n",
    "                predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "                predicted_tokens = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "                train_pred_sentences.append(predicted_tokens.split())\n",
    "                predicted_tokens = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "                train_true_sentences.append(predicted_tokens.split())\n",
    "\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id\n",
    "                loss += CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                train_percentage_matched += percentage\n",
    "                train_percentage_matched_ct += 1\n",
    "\n",
    "                # Backpropagate losses every GRADIENT_ACCUMULATION_STEPS or at the end of the dataset\n",
    "                if (idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or idx == len(train_articles) - 1:\n",
    "                    (loss / GRADIENT_ACCUMULATION_STEPS).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = 0\n",
    "            \n",
    "            print(\"Train : % Exact Match: \",train_percentage_matched/train_percentage_matched_ct)\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(train_true_sentences, train_pred_sentences)\n",
    "                print(f'Train BLEU Score: {bleu_score}')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_pred_sentences = []\n",
    "        val_true_sentences = []\n",
    "        with torch.no_grad():\n",
    "            val_percentage_matched = 0\n",
    "            val_percentage_matched_ct = 0\n",
    "            for article, summary in tqdm(zip(val_articles, val_summaries), total=len(val_articles), desc=\"Validation\", unit=\"batch\"):\n",
    "                input_ids = torch.tensor(article).to(device)\n",
    "                labels = torch.tensor(summary).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                # Bleu Score\n",
    "                pred_logits = outputs.logits\n",
    "                predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "                predicted_tokens = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "                val_pred_sentences.append(predicted_tokens.split())\n",
    "                predicted_tokens = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "                val_true_sentences.append(predicted_tokens.split())\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "                val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                val_percentage_matched += percentage\n",
    "                val_percentage_matched_ct += 1\n",
    "        \n",
    "        \n",
    "        print(\"Val : % Exact Match: \",val_percentage_matched/val_percentage_matched_ct)\n",
    "        avg_val_loss = total_val_loss / len(val_articles)\n",
    "        print(\"Val Loss : \",avg_val_loss)\n",
    "        bleu_score = corpus_bleu(val_true_sentences, val_pred_sentences)\n",
    "        print(f'Val BLEU Score: {bleu_score}')\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_pred_sentences = []\n",
    "    test_true_sentences = []\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_percentage_matched = 0\n",
    "        test_percentage_matched_ct = 0\n",
    "        for article, summary in tqdm(zip(test_articles, test_summaries), total=len(test_articles), desc=\"Validation\", unit=\"batch\"):\n",
    "            input_ids = torch.tensor(article).to(device)\n",
    "            labels = torch.tensor(summary).to(device)\n",
    "            outputs = model(input_ids, prompt_id)\n",
    "\n",
    "            # Bleu Score\n",
    "            pred_logits = outputs.logits\n",
    "            predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "            predicted_tokens = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "            test_pred_sentences.append(predicted_tokens.split())\n",
    "            predicted_tokens = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "            test_true_sentences.append(predicted_tokens.split())\n",
    "\n",
    "            ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "            test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            # Metrics\n",
    "            set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            set2 = set(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate the intersection of sets\n",
    "            intersection = set1.intersection(set2)\n",
    "\n",
    "            # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "            percentage = (len(intersection) / len(set1)) * 100\n",
    "            test_percentage_matched += percentage\n",
    "            test_percentage_matched_ct += 1\n",
    "    \n",
    "        \n",
    "        print(\"Test : % Exact Match: \",test_percentage_matched/test_percentage_matched_ct)\n",
    "        avg_test_loss = total_test_loss / len(test_articles)\n",
    "        print(\"Test Loss : \",avg_test_loss)\n",
    "        bleu_score = corpus_bleu(test_true_sentences, test_pred_sentences)\n",
    "        print(f'Test BLEU Score: {bleu_score}')\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "fine_tuned_model = fine_tune_on_summarization(model, tokenized_articles_train, tokenized_summaries_train, tokenized_articles_validation, tokenized_summaries_validation, tokenized_articles_test, tokenized_summaries_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(fine_tuned_model.state_dict(), '3.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2WithSoftPrompt(\n",
       "  (gpt2): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (soft_prompt): Embedding(1, 768)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new instance of the model\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n",
    "\n",
    "# Load the saved model state_dict\n",
    "model.load_state_dict(torch.load('3.pth'))\n",
    "\n",
    "# Make sure the model is in evaluation mode after loading\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Madam President, on a point of order.\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.... the.. the.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "PROMPT_TOKEN = \"Translate the following sentence from english to german :\"\n",
    "MAX_LEN = 500\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"Translate\",\"the\",\"following\",\"sentence\",\"from\",\"english\",\"to\",\"german\",\":\"]  # Define your custom vocabulary here\n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "\n",
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def load_data_from_files(english_file, german_file):\n",
    "    with open(english_file, \"r\", encoding=\"utf-8\") as eng_file:\n",
    "        english_list = eng_file.readlines()\n",
    "    \n",
    "    with open(german_file, \"r\", encoding=\"utf-8\") as ger_file:\n",
    "        german_list = ger_file.readlines()\n",
    "\n",
    "    return english_list[:1500], german_list[:150]  \n",
    "\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(english_file, german_file, num_prompts):\n",
    "    english_list, german_list = load_data_from_files(english_file, german_file)\n",
    "\n",
    "    # Perform preprocessing on the data\n",
    "    tokenized_english = []\n",
    "    tokenized_german = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for english_sentence, german_sentence in zip(english_list, german_list):\n",
    "        english_tokens = tokenizer.encode(english_sentence, truncation=True, max_length=MAX_LEN)\n",
    "        german_tokens = tokenizer.encode(german_sentence, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "        # Pad the sequences to MAX_LEN\n",
    "        padded_english = english_tokens + [tokenizer.eos_token_id] * (MAX_LEN - len(english_tokens))\n",
    "        padded_german = german_tokens + [tokenizer.eos_token_id] * (MAX_LEN+9 - len(german_tokens))\n",
    "\n",
    "        tokenized_english.append(padded_english)\n",
    "        tokenized_german.append(padded_german)\n",
    "\n",
    "    return tokenized_english, tokenized_german\n",
    "\n",
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenized_articles_total , tokenized_summaries_total = load_and_preprocess_data(\"europarl-v7.de-en.en\", \"europarl-v7.de-en.de\",num_prompts)\n",
    "total_samples = len(tokenized_articles_total)\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "# Calculate the sizes of the three sets\n",
    "train_size = int(total_samples * split_ratio[0])\n",
    "val_size = int(total_samples * split_ratio[1])\n",
    "test_size = int(total_samples * split_ratio[2])\n",
    "\n",
    "# Split the data\n",
    "tokenized_articles_train = tokenized_articles_total[:train_size]\n",
    "tokenized_summaries_train = tokenized_summaries_total[:train_size]\n",
    "\n",
    "tokenized_articles_validation = tokenized_articles_total[train_size:train_size + val_size]\n",
    "tokenized_summaries_validation = tokenized_summaries_total[train_size:train_size + val_size]\n",
    "\n",
    "tokenized_articles_test = tokenized_articles_total[train_size + val_size:]\n",
    "tokenized_summaries_test = tokenized_summaries_total[train_size + val_size:]\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "# # Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_pred_sentences = []\n",
    "test_true_sentences = []\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    test_percentage_matched = 0\n",
    "    test_percentage_matched_ct = 0\n",
    "    for article, summary in tqdm(zip(tokenized_articles_test, tokenized_summaries_test), total=len(tokenized_articles_test), desc=\"Validation\", unit=\"batch\"):\n",
    "        input_ids = torch.tensor(article).to(device)\n",
    "        labels = torch.tensor(summary).to(device)\n",
    "        outputs = model(input_ids, prompt_id)\n",
    "\n",
    "        # Bleu Score\n",
    "        pred_logits = outputs.logits\n",
    "        predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "        predicted_tokens = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "        test_pred_sentences.append(predicted_tokens.split())\n",
    "        predicted_tokens = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "        test_true_sentences.append(predicted_tokens.split())\n",
    "\n",
    "        ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "        test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "        # Metrics\n",
    "        set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        set2 = set(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate the intersection of sets\n",
    "        intersection = set1.intersection(set2)\n",
    "\n",
    "        # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "        percentage = (len(intersection) / len(set1)) * 100\n",
    "        test_percentage_matched += percentage\n",
    "        test_percentage_matched_ct += 1\n",
    "\n",
    "\n",
    "print(\"Test : % Exact Match: \",test_percentage_matched/test_percentage_matched_ct)\n",
    "avg_test_loss = total_test_loss / len(tokenized_summaries_test)\n",
    "print(\"Test Loss : \",avg_test_loss)\n",
    "bleu_score = corpus_bleu(test_true_sentences, test_pred_sentences)\n",
    "print(f'Test BLEU Score: {bleu_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
