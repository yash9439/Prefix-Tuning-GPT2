{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "PROMPT_TOKEN = \"[QUESTIONANSWERING]\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"[QUESTIONANSWERING]\"]  \n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "\n",
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "# Load data from a JSON file\n",
    "def load_data_from_json(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    context_list = [dp[\"context\"] for item in data[\"data\"] for dp in item[\"paragraphs\"]]\n",
    "    question_list = [dp[\"qas\"][0][\"question\"] for item in data[\"data\"] for dp in item[\"paragraphs\"]]\n",
    "    answer_list = [dp[\"qas\"][0][\"answers\"][0][\"text\"] if dp[\"qas\"] and dp[\"qas\"][0][\"answers\"] else \"\" for item in data[\"data\"] for dp in item[\"paragraphs\"]]\n",
    "\n",
    "    return context_list[:500], question_list[:500], answer_list[:500]\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(json_file, num_prompts):\n",
    "    context_list, question_list, answer_list = load_data_from_json(json_file)\n",
    "\n",
    "    # Perform preprocessing on the data\n",
    "    tokenized_question = []\n",
    "    tokenized_answer = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for context, question, answer in zip(context_list, question_list, answer_list):\n",
    "        # Tokenize context, question, and answer using the GPT-2 tokenizer\n",
    "        context_tokens = tokenizer.encode(context, truncation=True, max_length=MAX_LEN)\n",
    "        question_tokens = tokenizer.encode(question, truncation=True, max_length=MAX_LEN)\n",
    "        answer_tokens = tokenizer.encode(answer, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "        padded_article = question_tokens + [tokenizer.eos_token_id] * (MAX_LEN-1 - len(question_tokens))\n",
    "        padded_summary = answer_tokens + [tokenizer.eos_token_id] * (MAX_LEN - len(answer_tokens))\n",
    "\n",
    "        tokenized_question.append(padded_article)\n",
    "        tokenized_answer.append(padded_summary)\n",
    "\n",
    "    return tokenized_question, tokenized_answer\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenized_articles_train,tokenized_summaries_train = load_and_preprocess_data(\"train-v2.0.json\",num_prompts)\n",
    "tokenized_articles_validation,tokenized_summaries_validation = load_and_preprocess_data(\"dev-v2.0.json\", num_prompts)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# # Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/500 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 500/500 [01:05<00:00,  7.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.2524384559369075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.7082617382617378\n",
      "Val Loss :  11.22950127029419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 500/500 [01:00<00:00,  8.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  1.976042893008837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:18<00:00, 26.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.274280376486258\n",
      "Val Loss :  10.746776478767394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 500/500 [00:59<00:00,  8.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  1.9270479597801582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 26.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.3199665530547877\n",
      "Val Loss :  10.506160795211793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 500/500 [00:59<00:00,  8.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.0257935740318964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.3833564119507704\n",
      "Val Loss :  10.309152409553528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 500/500 [00:59<00:00,  8.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  1.9569456360632829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.599919100507335\n",
      "Val Loss :  10.044513963699341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 500/500 [01:00<00:00,  8.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.162822422675364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.6001054990760877\n",
      "Val Loss :  9.876015647888183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 500/500 [01:01<00:00,  8.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.272239704739704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.6334593035552096\n",
      "Val Loss :  9.733137884140014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 500/500 [01:02<00:00,  7.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.26721250971251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:20<00:00, 24.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.548849010466656\n",
      "Val Loss :  9.65255794429779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 500/500 [01:03<00:00,  7.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.4092756220387797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.622873874491521\n",
      "Val Loss :  9.587493703842163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 500/500 [01:01<00:00,  8.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  2.401964032699326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:19<00:00, 25.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  2.7092987567987565\n",
      "Val Loss :  9.532423812866211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "prompt_id = prompt_id.to(device)\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def fine_tune_on_summarization(model, train_articles, train_summaries, val_articles, val_summaries):\n",
    "    optimizer = torch.optim.Adam(model.soft_prompt.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        # Gradient accumulation initialization\n",
    "        optimizer.zero_grad()\n",
    "        accumulated_loss = 0\n",
    "        loss = 0\n",
    "        # Use tqdm for progress bar\n",
    "        with tqdm(enumerate(zip(train_articles, train_summaries)), total=len(train_articles), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n",
    "            train_percentage_matched = 0\n",
    "            train_percentage_matched_ct = 0\n",
    "            for idx, (article, summary) in progress:\n",
    "                input_ids = torch.tensor(article).to(device)\n",
    "                labels = torch.tensor(summary).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id\n",
    "                loss += CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                train_percentage_matched += percentage\n",
    "                train_percentage_matched_ct += 1\n",
    "\n",
    "                # Backpropagate losses every GRADIENT_ACCUMULATION_STEPS or at the end of the dataset\n",
    "                if (idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or idx == len(train_articles) - 1:\n",
    "                    (loss / GRADIENT_ACCUMULATION_STEPS).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = 0\n",
    "            \n",
    "            print(\"Train : % Exact Match: \",train_percentage_matched/train_percentage_matched_ct)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            val_percentage_matched = 0\n",
    "            val_percentage_matched_ct = 0\n",
    "            for article, summary in tqdm(zip(val_articles, val_summaries), total=len(val_articles), desc=\"Validation\", unit=\"batch\"):\n",
    "                input_ids = torch.tensor(article).to(device)\n",
    "                labels = torch.tensor(summary).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "                val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                val_percentage_matched += percentage\n",
    "                val_percentage_matched_ct += 1\n",
    "        \n",
    "        \n",
    "        print(\"Val : % Exact Match: \",val_percentage_matched/val_percentage_matched_ct)\n",
    "        avg_val_loss = total_val_loss / len(val_articles)\n",
    "        print(\"Val Loss : \",avg_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "fine_tuned_model = fine_tune_on_summarization(model, tokenized_articles_train, tokenized_summaries_train, tokenized_articles_validation, tokenized_summaries_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(fine_tuned_model.state_dict(), '2.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2WithSoftPrompt(\n",
       "  (gpt2): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (soft_prompt): Embedding(1, 768)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new instance of the model\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n",
    "\n",
    "# Load the saved model state_dict\n",
    "model.load_state_dict(torch.load('2.pth'))\n",
    "\n",
    "# Make sure the model is in evaluation mode after loading\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([360, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.. of..,... is. by woman,. the worldIs. the40s. the,s, \\'. as \"\\'filmve film the film ofs on out the 25, the death in the Hills, California.\\n\\'s who name was is \",uery, was a years was been been with. She fatherist, who Lman, was that news on that\\n down to the\\n\\n and \" K, born the processsa Bin filmJones film \"The Times Hard and Slow\\' and19) and the film film Lang film \\'The the City Sleeps\\' (. Francisco Times, who was a starge of the\\'sblazer anda Lupino, who was her in the roles in the such\\'film darling commercial success oforious, and Again, The, and and Beautiful.\\n of the\\'s other films roles include\\'man, The of a,, The Theite Me Love. which to the website-ovies.\\n.\\n film also says that\\'s a the filmsax films films Thelins, series of\\n\\'s in a in the episode of the Big Sullivan Show, the other of The Goodosaurs and. Chase. which firstMDB profile says.\\n also appeared in the number musical of the Big Dwar Itch and\\n of reports reports that she actresses actors include the You Like It, The One Not, Noette, The It,\\n\\'s her anddirectorcer and G, the. She died in a.\\n was survived by her husband, who,, who her,, who and John.eney.\\n\\n Forrest former Diego native, Forrest was a directorge of Hollywood legendblazer Ida Lupino in who was her in the roles in films such She'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3b798cf7-0a44-4faa-b2c5-42b34752a753)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "PROMPT_TOKEN = \"Answer the Following Question\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"Answer\",\"the\",\"Following\",\"Question\"]  \n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "\n",
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "# Load data from a JSON file\n",
    "def load_data_from_json(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    context_list = [dp[\"context\"] for item in data[\"data\"] for dp in item[\"paragraphs\"]]\n",
    "    question_list = [dp[\"qas\"][0][\"question\"] for item in data[\"data\"] for dp in item[\"paragraphs\"]]\n",
    "    answer_list = [dp[\"qas\"][0][\"answers\"][0][\"text\"] if dp[\"qas\"] and dp[\"qas\"][0][\"answers\"] else \"\" for item in data[\"data\"] for dp in item[\"paragraphs\"]]\n",
    "\n",
    "    return context_list[:50], question_list[:50], answer_list[:50]\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(json_file, num_prompts):\n",
    "    context_list, question_list, answer_list = load_data_from_json(json_file)\n",
    "\n",
    "    # Perform preprocessing on the data\n",
    "    tokenized_question = []\n",
    "    tokenized_answer = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for context, question, answer in zip(context_list, question_list, answer_list):\n",
    "        # Tokenize context, question, and answer using the GPT-2 tokenizer\n",
    "        context_tokens = tokenizer.encode(context, truncation=True, max_length=MAX_LEN)\n",
    "        question_tokens = tokenizer.encode(question, truncation=True, max_length=MAX_LEN)\n",
    "        answer_tokens = tokenizer.encode(answer, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "        padded_article = question_tokens + [tokenizer.eos_token_id] * (MAX_LEN - len(question_tokens))\n",
    "        padded_summary = answer_tokens + [tokenizer.eos_token_id] * (MAX_LEN+4 - len(answer_tokens))\n",
    "\n",
    "        tokenized_question.append(padded_article)\n",
    "        tokenized_answer.append(padded_summary)\n",
    "\n",
    "    return tokenized_question, tokenized_answer\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenized_articles_train,tokenized_summaries_train = load_and_preprocess_data(\"train-v2.0.json\",num_prompts)\n",
    "tokenized_articles_validation,tokenized_summaries_validation = load_and_preprocess_data(\"dev-v2.0.json\", num_prompts)\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "# # Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:24<00:00,  2.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  0.8412698412698413\n",
      "Val Loss :  11.775735130310059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# test\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    val_percentage_matched = 0\n",
    "    val_percentage_matched_ct = 0\n",
    "    for article, summary in tqdm(zip(tokenized_articles_validation, tokenized_summaries_validation), total=len(tokenized_articles_validation), desc=\"Validation\", unit=\"batch\"):\n",
    "        input_ids = torch.tensor(article).to(device)\n",
    "        labels = torch.tensor(summary).to(device)\n",
    "        outputs = model(input_ids, prompt_id)\n",
    "\n",
    "        ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "        val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        # Metrics\n",
    "        set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        set2 = set(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate the intersection of sets\n",
    "        intersection = set1.intersection(set2)\n",
    "\n",
    "        # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "        percentage = (len(intersection) / len(set1)) * 100\n",
    "        val_percentage_matched += percentage\n",
    "        val_percentage_matched_ct += 1\n",
    "\n",
    "\n",
    "print(\"Val : % Exact Match: \",val_percentage_matched/val_percentage_matched_ct)\n",
    "avg_val_loss = total_val_loss / len(tokenized_summaries_validation)\n",
    "print(\"Val Loss : \",avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
