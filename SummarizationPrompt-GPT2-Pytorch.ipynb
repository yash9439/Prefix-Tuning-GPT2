{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "PROMPT_TOKEN = \"[SUMMARIZE]\"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"[SUMMARIZE]\"]  # Define your custom vocabulary here\n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "\n",
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path, num_prompts):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna().sample(frac=0.001)  # Use only 10% of the data\n",
    "\n",
    "    # Perform preprocessing on the data\n",
    "    tokenized_articles = []\n",
    "    tokenized_summaries = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for article, summary in zip(df[\"article\"], df[\"highlights\"]):\n",
    "        # Adjust the maximum length of articles to avoid exceeding MAX_LEN\n",
    "        max_length_article = MAX_LEN - num_prompts \n",
    "        article_tokens = tokenizer.encode(article, truncation=True, max_length=max_length_article)\n",
    "        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=300)\n",
    "\n",
    "        max_length_summary = MAX_LEN\n",
    "        padded_article = article_tokens + [tokenizer.eos_token_id] * (max_length_article - len(article_tokens))\n",
    "        padded_summary = summary_tokens + [tokenizer.eos_token_id] * (max_length_summary - len(summary_tokens))\n",
    "\n",
    "        tokenized_articles.append(padded_article)\n",
    "        tokenized_summaries.append(padded_summary)\n",
    "\n",
    "\n",
    "    return tokenized_articles, tokenized_summaries\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenized_articles_train,tokenized_summaries_train = load_and_preprocess_data(\"cnn_dailymail/train.csv\", num_prompts)\n",
    "tokenized_articles_validation,tokenized_summaries_validation = load_and_preprocess_data(\"cnn_dailymail/validation.csv\", num_prompts)\n",
    "tokenized_articles_test,tokenized_summaries_test = load_and_preprocess_data(\"cnn_dailymail/test.csv\", num_prompts)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# # Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 287/287 [01:16<00:00,  3.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.785882918207484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 10.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  17.306666893832748\n",
      "Val Loss :  10.00016681964581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 287/287 [01:17<00:00,  3.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  15.878994127679539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 10.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  16.60374007772575\n",
      "Val Loss :  8.791603235098032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 287/287 [01:17<00:00,  3.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  15.985369812191355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00,  9.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  16.564957989310454\n",
      "Val Loss :  8.548761661236103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 287/287 [01:18<00:00,  3.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.115932752201587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 11.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  16.39624704438616\n",
      "Val Loss :  8.436102096851055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 287/287 [01:17<00:00,  3.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.361446681569646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 11.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  16.707541893935183\n",
      "Val Loss :  8.383278700021597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 287/287 [01:22<00:00,  3.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.45055023666308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 10.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  16.831990633783825\n",
      "Val Loss :  8.323648452758789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 287/287 [01:18<00:00,  3.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.567110821226272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 11.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  17.004581875011777\n",
      "Val Loss :  8.286963316110464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 287/287 [01:20<00:00,  3.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.6091242059968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00,  9.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  17.129143331716193\n",
      "Val Loss :  8.246843924889198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 287/287 [01:21<00:00,  3.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  16.765729049603802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00, 10.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  17.453579138879523\n",
      "Val Loss :  8.216329574584961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 287/287 [01:18<00:00,  3.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  17.02098513233092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:01<00:00,  9.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  17.61320407970948\n",
      "Val Loss :  8.189803380232592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 11/11 [00:01<00:00, 10.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : % Exact Match:  16.58805211617323\n",
      "Test Loss :  8.49093415520408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "prompt_id = prompt_id.to(device)\n",
    "# Import cross_entropy_loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def fine_tune_on_summarization(model, train_articles, train_summaries, val_articles, val_summaries, test_articles, test_summaries):\n",
    "    optimizer = torch.optim.Adam(model.soft_prompt.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        # Gradient accumulation initialization\n",
    "        optimizer.zero_grad()\n",
    "        accumulated_loss = 0\n",
    "        loss = 0\n",
    "        # Use tqdm for progress bar\n",
    "        with tqdm(enumerate(zip(train_articles, train_summaries)), total=len(train_articles), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n",
    "            train_percentage_matched = 0\n",
    "            train_percentage_matched_ct = 0\n",
    "            for idx, (article, summary) in progress:\n",
    "                input_ids = torch.tensor(article).to(device)\n",
    "                labels = torch.tensor(summary).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id\n",
    "                loss += CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                train_percentage_matched += percentage\n",
    "                train_percentage_matched_ct += 1\n",
    "\n",
    "                # Backpropagate losses every GRADIENT_ACCUMULATION_STEPS or at the end of the dataset\n",
    "                if (idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or idx == len(train_articles) - 1:\n",
    "                    (loss / GRADIENT_ACCUMULATION_STEPS).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = 0\n",
    "            \n",
    "            print(\"Train : % Exact Match: \",train_percentage_matched/train_percentage_matched_ct)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            val_percentage_matched = 0\n",
    "            val_percentage_matched_ct = 0\n",
    "            for article, summary in tqdm(zip(val_articles, val_summaries), total=len(val_articles), desc=\"Validation\", unit=\"batch\"):\n",
    "                input_ids = torch.tensor(article).to(device)\n",
    "                labels = torch.tensor(summary).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "                val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                val_percentage_matched += percentage\n",
    "                val_percentage_matched_ct += 1\n",
    "\n",
    "        print(\"Val : % Exact Match: \",val_percentage_matched/val_percentage_matched_ct)\n",
    "        avg_val_loss = total_val_loss / len(val_articles)\n",
    "        print(\"Val Loss : \",avg_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_percentage_matched = 0\n",
    "        test_percentage_matched_ct = 0\n",
    "        for article, summary in tqdm(zip(test_articles, test_summaries), total=len(test_articles), desc=\"Test\", unit=\"batch\"):\n",
    "            input_ids = torch.tensor(article).to(device)\n",
    "            labels = torch.tensor(summary).to(device)\n",
    "            outputs = model(input_ids, prompt_id)\n",
    "\n",
    "            ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "            test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            # Metrics\n",
    "            set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            set2 = set(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate the intersection of sets\n",
    "            intersection = set1.intersection(set2)\n",
    "\n",
    "            # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "            percentage = (len(intersection) / len(set1)) * 100\n",
    "            test_percentage_matched += percentage\n",
    "            test_percentage_matched_ct += 1\n",
    "        \n",
    "        \n",
    "        print(\"Test : % Exact Match: \",test_percentage_matched/test_percentage_matched_ct)\n",
    "        avg_test_loss = total_test_loss / len(test_articles)\n",
    "        print(\"Test Loss : \",avg_test_loss)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "fine_tuned_model = fine_tune_on_summarization(model, tokenized_articles_train, tokenized_summaries_train, tokenized_articles_validation, tokenized_summaries_validation, tokenized_articles_test,tokenized_summaries_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(fine_tuned_model.state_dict(), '1.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2WithSoftPrompt(\n",
       "  (gpt2): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (soft_prompt): Embedding(1, 768)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new instance of the model\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n",
    "\n",
    "# Load the saved model state_dict\n",
    "model.load_state_dict(torch.load('1.pth'))\n",
    "\n",
    "# Make sure the model is in evaluation mode after loading\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([364, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. the-, the..al the \",in.-\\'sas the \"-. the \"The-, theThe-. the\\'s., the the as the \"-.. the \" ofs and in the 1. the first in the Hills, the,\\n, the name in is the,isty, the a. the a- the.\\n nameist and and,,, the the death to.\\n. the the.\\n and \",, the the \"-a,in, the,,The to the and the, and19 to and the \\'- the, \\'The the film ofs, and. Francisco, and the, the.ge, the\\'sbla,a,in, the was the in the in in the like\\'\\' and the for of a, the a the the to the and the,\\n of the, \\',,, the,, the of the and, the theuse theray the to the,..,\\n.\\n first for has that, a the timesax,\\n the,, series of\\n, in the, the episode of the M and,, the of of the New\\'s,,,, the firstM....\\n was was in the series play of the M, It\\'s and\\n of,, that the people and for the a Can to, The. I, No,, the,,\\n\\'s the andincer and,, the.\\n was in the.\\n was the by her two, the,, and the,, the and the.eney.\\n\\n The. Francisco,, the was a memberge of the\\'sblazer,a Lupin, the was the in the in in the like She'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([781, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Celtic defender Virgil van Dijk admits he feared his cup final dream had been wrecked by last week's red card at Tannadice. The Dutchman was sent-off during the Hoops' William Hill Scottish Cup quarter-final draw with Dundee United following an off-the-ball spat with the Taysider's Calum Butcher. Referee Craig Thompson sent-off Van Dijk before making a major gaffe when he wrongly dismissed Butcher's Terrors team-mate Paul Paton. Virgil van Dijk (2nd left) goes up for a header during Celtic's 2-0 Scottish League Cup final win . Hoops boss Ronny Deila celebrates winning his first piece of silverware since arriving at Parkhead . That left both men sweating on their places for Sunday's QTS Scottish League Cup showdown at Hampden. But there was relief for Van Dijk and Paton as they were later cleared to play after successful appeals to the Scottish Football Association's disciplinary panel. And it was the Celtic centre-half who was left bearing the brightest grin as his side clinched Ronny Deila's first trophy as Parkhead boss with a 2-0 win at the National Stadium. Van Dijk said: 'It was quite a tough week. I've never experienced anything like it in my life. 'It would have been very disappointing if I'd not been allowed to play on Sunday, especially with it being my first final ever. 'But you know, justice was served and I was able to play. Luckily I got the red card overturned and we did a good job.' The Dutch defender was sent off for a clash with Callum Butcher in Scottish Cup quarter finals . Van Dijk is sent off but was later cleared to play on Sunday following a successful appeal . Dundee United's Paul Paton also had his red card rescinded after being wrongly sent off . Van Dijk has been linked with summer moves to south, with Barclays Premier League high-flyers Arsenal and Southampton monitoring his progress. But for now the 23-year-old is happy to enjoy Celtic's treble chase. 'It means a lot to have lifted the League Cup,' he said. 'It's my first cup trophy ever. 'This club is an amazing place, I have always said that. I have been improving since the day came here. 'That is the most important thing for a player. If you win trophies, that's even better.' Kris Commons fired Celtic ahead midway through the first half before substitute James Forrest stroked home a second 12 minutes from time. The Hoops winger also had time to miss a late penalty as a United side that spent the last 35 minutes a man down following skipper Sean Dillon's red card avoided a heavier defeat. However, Jackie McNamara's team will go for revenge when the sides meet for part three of their four-game duel with Wednesday's Scottish Cup replay. Van Dijk said: 'We made it tough for ourselves on Sunday and should have finished the game faster in the second half. James Forrest celebrates scoring Celtic's second goal before missing a late penalty at Hampden . Scorer of Celtic's first goal, Kris Commons, celebrates with the trophy following Celtic's win . 'One-nil is a dangerous score - if they had scored one goal they would have had the believe to hit us with everything. 'At moments it looked tough but Craig Gordon only had one save to make in the first half and nothing in the second half, so I think we did well. 'It's a big boost for us ahead of Wednesday night. We can go for the second cup now full of confidence. 'They will be up for it on Wednesday night and know they have possibilities with the players coming back in to their team. 'But we need to be ready and win the game.\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. the-, the....gil. der.. to\\'s for team was. of a a by the year. \"-. the.adon.\\n \"man, a toin. the firstof. first \". and.-final.. theee.. the early-the-c. with the \"ann.. ined..\\n.: T.-B the D.. the a play,affe. the was called the\\'s callri..in, D O.\\ngil van Dijk.R- from the and on for the 2. the\\'s first-1 win Cup Cup...\\nops\\',an De G. the the first- of theware. the in the..\\n\\'s the the in. the way in the\\'s 2.. Cup Cup.. theden.\\n the was a for the Dijk. theon. the were the in of the the the draw. the T Cup League. decision committee.\\n the was a first\\'s-in\\'s was the in the \" of. he team\\'sched theny Deila\\'s place-. thehead\\'s. a 2-1 win. the end..\\n Dijk\\'s the \"I was a a bit one for I was been seen a like that. the life. I\\n was be been a,. I had not been in to play. the. but with the being the first time.. \\'I I can, I is served. the was a to get. \\', was the chance card.. the were. good job of\\n gameman, the off for a red with the of,. the Cup.-. The Dijk\\'s the off for the not cleared to play.-. the 2 appeal. Theee United\\'s first Don was was a first card.ed. the sent dismissed off. The Dijk\\'s a sent with a transfer to the of and the. League and-backer the and the. the future.\\n the the, Dutch-year-old is in to be the\\'s firstble. for\\nI\\'s a lot to me a the trophy Cup. he said. \\'I\\'s a first time in.. \\'I is is a important club to and\\'m a been..\\'have a a and I start I.. \\'I\\'s the first important thing. me player to\\'I have the, you is the more.\\n van, the\\'s of through the second half of the D O wasde the a free.- from the. The homeops were, had a to get the penalty goal. the result player. had the first two minutes of half.. aipper\\'s O\\'s late card. a penalty penalty. The, the Oamara\\'s side- be on a. the game meet in the of of the first-match home. the\\'s 2 Cup final. The Dijk\\'s: \\'I are a to. the. the. I have been the game.. the second half.\\'Forrest\\'s his the\\'s first goal in the the penalty penalty. theden. \\'ored D the\\'s second- of the Commons, was his the ball after a\\'s 2 over TheI ofon to a lot goal. it you are a it,, would have been a lead that go the. a they \\'I the I was like, we Thompson\\'s had a chance. go it the first half. the to the second half. so it was we were a. \\'I\\'s a great win for the. of the\\'s.\\'have\\'t to the second half and. of confidence and \\'I are be a for the. Wednesday.. I that have a. the game. in. the the team. \\'I I have to be more for we the game. \\''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "PROMPT_TOKEN = \"Summarize the following sentence :\"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"Summarize\", \"the\", \"following\", \"sentence\", \":\"]  # Define your custom vocabulary here\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "\n",
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path, num_prompts):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna().sample(frac=0.0001)  # Use only 10% of the data\n",
    "\n",
    "    # Perform preprocessing on the data\n",
    "    tokenized_articles = []\n",
    "    tokenized_summaries = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for article, summary in zip(df[\"article\"], df[\"highlights\"]):\n",
    "        # Adjust the maximum length of articles to avoid exceeding MAX_LEN\n",
    "        max_length_article = MAX_LEN - num_prompts \n",
    "        article_tokens = tokenizer.encode(article, truncation=True, max_length=max_length_article)\n",
    "        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=300)\n",
    "\n",
    "        max_length_summary = MAX_LEN\n",
    "        padded_article = article_tokens + [tokenizer.eos_token_id] * (max_length_article - len(article_tokens))\n",
    "        padded_summary = summary_tokens + [tokenizer.eos_token_id] * (max_length_summary - len(summary_tokens))\n",
    "\n",
    "        tokenized_articles.append(padded_article)\n",
    "        tokenized_summaries.append(padded_summary)\n",
    "\n",
    "\n",
    "    return tokenized_articles, tokenized_summaries\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenized_articles_train,tokenized_summaries_train = load_and_preprocess_data(\"cnn_dailymail/train.csv\", num_prompts)\n",
    "tokenized_articles_validation,tokenized_summaries_validation = load_and_preprocess_data(\"cnn_dailymail/validation.csv\", num_prompts)\n",
    "tokenized_articles_test,tokenized_summaries_test = load_and_preprocess_data(\"cnn_dailymail/test.csv\", num_prompts)\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "# # Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1/1 [00:01<00:00,  1.36s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : % Exact Match:  19.318181818181817\n",
      "Test Loss :  11.197511672973633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    test_percentage_matched = 0\n",
    "    test_percentage_matched_ct = 0\n",
    "    for article, summary in tqdm(zip(tokenized_articles_test, tokenized_summaries_test), total=len(tokenized_articles_test), desc=\"Test\", unit=\"batch\"):\n",
    "        input_ids = torch.tensor(article).to(device)\n",
    "        labels = torch.tensor(summary).to(device)\n",
    "        outputs = model(input_ids, prompt_id)\n",
    "\n",
    "        ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "        test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "        # Metrics\n",
    "        set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        set2 = set(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate the intersection of sets\n",
    "        intersection = set1.intersection(set2)\n",
    "\n",
    "        # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "        percentage = (len(intersection) / len(set1)) * 100\n",
    "        test_percentage_matched += percentage\n",
    "        test_percentage_matched_ct += 1\n",
    "    \n",
    "    \n",
    "    print(\"Test : % Exact Match: \",test_percentage_matched/test_percentage_matched_ct)\n",
    "    avg_test_loss = total_test_loss / len(tokenized_articles_test)\n",
    "    print(\"Test Loss : \",avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
